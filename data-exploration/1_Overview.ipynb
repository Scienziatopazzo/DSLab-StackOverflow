{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 't' from 'sqlalchemy_utils' (/Users/clemens/.pyenv/versions/3.7.0/envs/heckoverflow/lib/python3.7/site-packages/sqlalchemy_utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-852e767e3776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 't' from 'sqlalchemy_utils' (/Users/clemens/.pyenv/versions/3.7.0/envs/heckoverflow/lib/python3.7/site-packages/sqlalchemy_utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from sqlalchemy_utils import analyze\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy.stats import expon\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "postgres_str = ('postgresql://localhost/crossvalidated')\n",
    "cnx = create_engine(postgres_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>I want to fit a function $f(x_1,x_2..)$ to (noisy) data with unknown variance. For each datapoint, I have a weight $w_i$ which is proportional to the reliability of that particular datapoint. The real uncertainty of the datapoints is unknown.\\nI do the fitting using Matlab and Levenberg-Marquardt so that I end up with the parameter estimates $\\\\hat{\\\\beta}$ and the Jacobian $J$. I calculate the variance of the parameter estimates as follows:\\n$$\\\\hat{\\\\sigma}^2=\\\\frac{RSS}{n-p+1} \\\\cdot (J'WJ)^{-1}$$ \\nWhere $W$ is the diagonal matrix of the weights, $n$ is the number of datapoints and $p$ is the degrees of freedom.</p>\\n\\n<p>My question is regarding the normalization of the weights and it's effect on the parameter uncertainty, since I have seen some conflicting definitions and opinions on this subject.</p>\\n\\n<p><strong>Question</strong>: How do I normalize my weights so that the calculated parameter standard errors are meaningful?\\nI currently normalize the weights so that their sum is the number of datapoints $N$, similar to non-weighted least squares: $\\\\Sigma w_i = N$. Is this correct?</p>\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_sql_query('''SELECT body FROM Posts LIMIT 5;''', cnx)\n",
    "tmp.body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nested Loop', 'Index Scan', 'Index Scan']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis = analyze(cnx, sqlalchemy.sql.text('SELECT Q.Id as question_id, A.Id as answer_id, A.OwnerUserId as answerer_user_id         FROM Posts A INNER JOIN Posts Q on A.ParentId = Q.Id         WHERE A.ParentId IN (23, 44) AND (Q.AcceptedAnswerId = A.Id OR A.Score >= 3)'))\n",
    "analysis.node_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Various measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_per_year = pd.read_sql_query(\n",
    "                    '''SELECT CAST(date_part('year', CreationDate) AS INTEGER) AS Year, COUNT(*) AS Post_n\n",
    "                    FROM Posts\n",
    "                    GROUP BY date_part('year', CreationDate);''', cnx)\n",
    "sns.lineplot(x=posts_per_year['year'], y=posts_per_year['post_n']).set_title('Posts per year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_per_year = pd.read_sql_query(\n",
    "                    '''SELECT CAST(date_part('year', CreationDate) AS INTEGER) AS Year, COUNT(*) AS qst_n\n",
    "                    FROM Posts\n",
    "                    WHERE PostTypeId=1\n",
    "                    GROUP BY date_part('year', CreationDate);''', cnx)\n",
    "sns.lineplot(x=questions_per_year['year'], y=questions_per_year['qst_n']).set_title('Questions per year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newusers_per_year = pd.read_sql_query(\n",
    "                    '''SELECT CAST(date_part('year', CreationDate) AS INTEGER) AS Year, COUNT(*) AS User_n\n",
    "                    FROM Users\n",
    "                    GROUP BY date_part('year', CreationDate);''', cnx)\n",
    "sns.lineplot(x=newusers_per_year['year'], y=newusers_per_year['user_n']).set_title('New users per year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ans_per_year = pd.read_sql_query(\n",
    "        '''SELECT Year, AVG(c_answers) AS Avg_answers FROM (\n",
    "        SELECT Q.Id, CAST(date_part('year', Q.CreationDate) AS INTEGER) AS Year, COUNT(Ans.Id) AS c_answers\n",
    "        FROM Posts AS Q LEFT JOIN Posts AS Ans ON Q.Id=Ans.ParentId\n",
    "        WHERE Q.PostTypeId=1\n",
    "        GROUP BY Q.Id) AS anstable\n",
    "        GROUP BY Year\n",
    "        ORDER BY Year ASC;''', cnx)\n",
    "sns.lineplot(x=avg_ans_per_year['year'], y=avg_ans_per_year['avg_answers']).set_title('Average number of answers to a question per year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of users that wrote more than x answers:\")\n",
    "for i in [1,2,5,10]:\n",
    "    avg_ans_per_year = pd.read_sql_query(\n",
    "            '''with countPerUser as (Select OwnerUserId, count(OwnerUserId) as c from Posts where ParentId is not null group by OwnerUserId)\n",
    "    Select (Select CAST(Count(c) AS float) from countPerUser where c>%i)/CAST(Count(c) AS float) from countPerUser'''%i, cnx)\n",
    "    print(\"Users with more than \", i, \" answers: \", avg_ans_per_year.values[0,0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of users that wrote more than x questions:\")\n",
    "for i in [1,2,5,10]:\n",
    "    avg_ans_per_year = pd.read_sql_query(\n",
    "            '''with countPerUser as (Select OwnerUserId, count(OwnerUserId) as c from Posts where ParentId is null group by OwnerUserId)\n",
    "    Select (Select CAST(Count(c) AS float) from countPerUser where c>%i)/CAST(Count(c) AS float) from countPerUser'''%i, cnx)\n",
    "    print(\"Users with more than \", i, \" question: \", avg_ans_per_year.values[0,0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_acc_ans = pd.read_sql_query(\n",
    "        '''SELECT Q.Id, Q.CreationDate, (Ans.CreationDate - Q.CreationDate) AS TimeToAns\n",
    "        FROM Posts AS Q LEFT JOIN Posts AS Ans ON Q.AcceptedAnswerId=Ans.Id\n",
    "        WHERE Q.PostTypeId=1;''', cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_acc_ans['timetoans_bin'] = 'never'\n",
    "time_to_acc_ans.loc[time_to_acc_ans['timetoans'] <= pd.Timedelta('1 hour'), 'timetoans_bin'] = '<1 hour'\n",
    "time_to_acc_ans.loc[\n",
    "    (time_to_acc_ans['timetoans'] > pd.Timedelta('1 hour')) & (time_to_acc_ans['timetoans'] <= pd.Timedelta('1 day')),\n",
    "    'timetoans_bin'] = '<1 day'\n",
    "time_to_acc_ans.loc[\n",
    "    (time_to_acc_ans['timetoans'] > pd.Timedelta('1 day')) & (time_to_acc_ans['timetoans'] <= pd.Timedelta('7 days')),\n",
    "    'timetoans_bin'] = '<1 week'\n",
    "time_to_acc_ans.loc[\n",
    "    (time_to_acc_ans['timetoans'] > pd.Timedelta('7 days')) & (time_to_acc_ans['timetoans'] <= pd.Timedelta('30 days')),\n",
    "    'timetoans_bin'] = '<1 month'\n",
    "time_to_acc_ans.loc[\n",
    "    (time_to_acc_ans['timetoans'] > pd.Timedelta('30 days')) & (time_to_acc_ans['timetoans'] <= pd.Timedelta('365 days')),\n",
    "    'timetoans_bin'] = '<1 year'\n",
    "time_to_acc_ans.loc[time_to_acc_ans['timetoans'] > pd.Timedelta('365 days'), 'timetoans_bin'] = '>1 year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"timetoans_bin\", data=time_to_acc_ans, order=['<1 hour', '<1 day', '<1 week', '<1 month', '<1 year', '>1 year', 'never']).set_title('Time to accepted answer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_first_ans = pd.read_sql_query(\n",
    "        '''SELECT Q.Id, Q.CreationDate, MIN(Ans.CreationDate - Q.CreationDate) AS TimeToAns\n",
    "        FROM Posts AS Q LEFT JOIN Posts AS Ans ON Q.Id=Ans.ParentId\n",
    "        WHERE Q.PostTypeId=1\n",
    "        GROUP BY Q.Id;''', cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_first_ans['timetoans_bin'] = 'never'\n",
    "time_to_first_ans.loc[time_to_first_ans['timetoans'] <= pd.Timedelta('1 hour'), 'timetoans_bin'] = '<1 hour'\n",
    "time_to_first_ans.loc[\n",
    "    (time_to_first_ans['timetoans'] > pd.Timedelta('1 hour')) & (time_to_first_ans['timetoans'] <= pd.Timedelta('1 day')),\n",
    "    'timetoans_bin'] = '<1 day'\n",
    "time_to_first_ans.loc[\n",
    "    (time_to_first_ans['timetoans'] > pd.Timedelta('1 day')) & (time_to_first_ans['timetoans'] <= pd.Timedelta('7 days')),\n",
    "    'timetoans_bin'] = '<1 week'\n",
    "time_to_first_ans.loc[\n",
    "    (time_to_first_ans['timetoans'] > pd.Timedelta('7 days')) & (time_to_first_ans['timetoans'] <= pd.Timedelta('30 days')),\n",
    "    'timetoans_bin'] = '<1 month'\n",
    "time_to_first_ans.loc[\n",
    "    (time_to_first_ans['timetoans'] > pd.Timedelta('30 days')) & (time_to_first_ans['timetoans'] <= pd.Timedelta('365 days')),\n",
    "    'timetoans_bin'] = '<1 year'\n",
    "time_to_first_ans.loc[time_to_first_ans['timetoans'] > pd.Timedelta('365 days'), 'timetoans_bin'] = '>1 year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"timetoans_bin\", data=time_to_first_ans, order=['<1 hour', '<1 day', '<1 week', '<1 month', '<1 year', '>1 year', 'never']).set_title('Time to first answer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_first_ans.groupby(['creationyear', 'timetoans_bin']).size().reset_index()\\\n",
    "    .pivot(index='timetoans_bin', columns='creationyear', values=0)\\\n",
    "    .reindex(['<1 hour', '<1 day', '<1 week', '<1 month', '<1 year', '>1 year', 'never']).T\\\n",
    "    .plot(kind='bar', stacked=True).set_title('Time to first answer over the years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time between first answer and accepted Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time first answer to accepted answer Q.Id, Q.CreationDate, MIN(Ans.CreationDate - Q.CreationDate) AS TimeToAns\n",
    "q = '''SELECT Q.Id as qid, Accepted_Ans.CreationDate - MIN(Ans.CreationDate) as first_ans_to_accepted_ans, EXTRACT(YEAR FROM Q.CreationDate) as creationyear\n",
    "        FROM Posts AS Q INNER JOIN Posts AS Ans ON Q.Id=Ans.ParentId\n",
    "        LEFT JOIN Posts AS Accepted_Ans on Q.AcceptedAnswerId = Accepted_Ans.Id\n",
    "        WHERE Q.PostTypeId=1 GROUP BY Q.Id, Accepted_Ans.CreationDate\n",
    "        '''\n",
    "first_to_accepted_answer = pd.read_sql_query(q, cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_df_with_timebins(df, colname):\n",
    "    bin_col_name = \"{}_bin\".format(colname)\n",
    "    \n",
    "    df[bin_col_name] = 'never'\n",
    "    df.loc[df[colname] == pd.Timedelta(0), bin_col_name] = 'same'\n",
    "    df.loc[(df[colname] <= pd.Timedelta('1 hour')) & (df[colname] != pd.Timedelta(0)),\n",
    "           bin_col_name] = '<1 hour'\n",
    "    df.loc[\n",
    "        (df[colname] > pd.Timedelta('1 hour')) & (df[colname] <= pd.Timedelta('1 day')),\n",
    "        bin_col_name] = '<1 day'\n",
    "    df.loc[\n",
    "        (df[colname] > pd.Timedelta('1 day')) & (df[colname] <= pd.Timedelta('7 days')),\n",
    "        bin_col_name] = '<1 week'\n",
    "    df.loc[\n",
    "        (df[colname] > pd.Timedelta('7 days')) & (df[colname] <= pd.Timedelta('30 days')),\n",
    "        bin_col_name] = '<1 month'\n",
    "    df.loc[\n",
    "        (df[colname] > pd.Timedelta('30 days')) & (df[colname] <= pd.Timedelta('365 days')),\n",
    "        bin_col_name] = '<1 year'\n",
    "    df.loc[df[colname] > pd.Timedelta('365 days'), bin_col_name] = '>1 year'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_to_accepted_with_bins = extend_df_with_timebins(first_to_accepted_answer, \"first_ans_to_accepted_ans\")\n",
    "#first_to_accepted_with_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.countplot(x=\"first_ans_to_accepted_ans_bin\", \n",
    "              data=first_to_accepted_with_bins, \n",
    "              order=['same', '<1 hour', '<1 day', '<1 week', '<1 month', '<1 year', '>1 year', 'never']).set_title('Time from first answer to accepted answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_to_accepted_with_bins.groupby(['creationyear', 'first_ans_to_accepted_ans_bin']).size().reset_index()\\\n",
    "    .pivot(index='first_ans_to_accepted_ans_bin', columns='creationyear', values=0)\\\n",
    "    .reindex(['same', '<1 hour', '<1 day', '<1 week', '<1 month', '<1 year', '>1 year', 'never']).T\\\n",
    "    .plot(kind='bar', stacked=True, figsize=(10,8.27)).set_title('Time to first answer over the years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Users Age when they answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "    SELECT A.ParentId as question_id, U.Id as answerer_id, COALESCE(A.Id = Q.AcceptedAnswerId, FALSE) accepted, A.CreationDate as date_of_answer, (A.CreationDate - Q.CreationDate) as age_of_question, A.CreationDate - U.CreationDate as user_age_at_answer_time\n",
    "    FROM Posts A INNER JOIN Users U ON A.OwnerUserId = U.Id \n",
    "    INNER JOIN Posts Q ON A.ParentId = Q.Id\n",
    "    WHERE A.PostTypeId = 2 \n",
    "\"\"\"\n",
    "\n",
    "user_age_at_answer_times = pd.read_sql_query(q, cnx)\n",
    "\n",
    "# can be negative since some answers have a time in the database dump that is obviously wrong\n",
    "# https://data.stackexchange.com/stats/query/1137375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_age_at_answer_times[user_age_at_answer_times.user_age_at_answer_time > pd.Timedelta(0)].sort_values(\"user_age_at_answer_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_age_in_days = user_age_at_answer_times.user_age_at_answer_time / pd.Timedelta('1 day')\n",
    "question_age_in_days = user_age_at_answer_times.age_of_question / pd.Timedelta('1 day')\n",
    "\n",
    "user_age_accepted = user_age_in_days[user_age_at_answer_times.accepted]\n",
    "user_age_not_accepted = user_age_in_days[np.invert(user_age_at_answer_times.accepted)]\n",
    "\n",
    "_ = plt.hist([user_age_accepted, user_age_not_accepted], bins=np.linspace(0, 3200, 30), label=['accepted', 'not_accepted'], log=True)\n",
    "plt.xlabel(\"Age at answer time in days\")\n",
    "plt.ylabel(\"Count [log]\")\n",
    "plt.legend()\n",
    "# plt.scatter(x=question_age_in_days, y=user_age_in_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze which tags are occuring most often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.read_sql_query(\n",
    "        '''SELECT Tags FROM Posts''', cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_posts = len(tags)\n",
    "none_counter = 0\n",
    "tag_list = []\n",
    "for t in tags.values:\n",
    "    if t[0] is not None:\n",
    "        tags_this_post = t[0].split(\"><\")\n",
    "        for tag in tags_this_post:\n",
    "            tag = tag.replace(\"<\", \"\")\n",
    "            tag = tag.replace(\">\", \"\")\n",
    "            tag_list.append(tag)\n",
    "    else:\n",
    "        none_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tag_list))\n",
    "unique_tags, counts = np.unique(tag_list, return_counts=True)\n",
    "sorted_counts_inds = np.argsort(counts)\n",
    "sorted_tags = unique_tags[sorted_counts_inds]\n",
    "sorted_counts = counts[sorted_counts_inds]\n",
    "print(sorted_tags[-10:], sorted_counts[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted_tags[-10:]\n",
    "data = sorted_counts[-10:]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(np.arange(1,11,1), data)\n",
    "plt.xticks(np.arange(1,11,1), labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further information about tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NUMBER OF DIFFERENT TAGS:\", len(unique_tags))\n",
    "print(\"PERCENTAGE OF TAGS OCCURING MORE THAN ONCE: \", 100*len(unique_tags[counts>1])/len(unique_tags), \"%\")\n",
    "print(\"PERCENTAGE OF TAGS OCCURING MORE THAN 100 TIMES: \", 100*len(unique_tags[counts>100])/len(unique_tags), \"%\")\n",
    "print(\"PERCENTAGE OF POSTS WITHOUT ANY TAGS:\", 100*none_counter/number_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to fit distribution to question_creation - answer values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all questionage values (for each answer, get how old the corresponding question was when it was answered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questionage_table = pd.read_sql_query('''SELECT a.id, (answercreationdate-CreationDate) as questionage FROM \n",
    "(SELECT parentid as Id, creationdate as answercreationdate FROM Posts WHERE PostTypeId=2) a\n",
    "LEFT JOIN Posts b ON a.Id=b.Id;''', cnx) # On AS Q LEFT JOIN Posts AS Ans\n",
    "questionage_table[\"questionage\"] = questionage_table[\"questionage\"].dt.days +  (questionage_table[\"questionage\"].dt.seconds)/(24*60*60)\n",
    "questionage_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('''(SELECT parentid as Id, creationdate as answercreationdate FROM Posts WHERE PostTypeId=2 LIMIT 5);''', cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_vals = questionage_table[\"questionage\"].values\n",
    "age_vals = age_vals[age_vals>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(age_vals[age_vals<10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit scipy distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = st.gilbrat.fit(age_vals[age_vals<500].astype(np.float64))\n",
    "print(params)\n",
    "plt.plot(x, eval(dist).pdf(x, params[0], params[1]),'r-', lw=5, alpha=0.6, label=dist, c=cols[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"green\", \"red\", \"purple\", \"yellow\"]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(age_vals[age_vals<100], bins=50, density=True)\n",
    "for i, dist in enumerate([\"st.expon\", \"st.pareto\", \"st.powerlaw\", \"st.gilbrat\"]):\n",
    "    params = eval(dist).fit(age_vals[age_vals<100].astype(np.float64))\n",
    "    print(params)\n",
    "    if dist == \"st.gilbrat\":\n",
    "        params = (-0.3530395997092245, 1.3032193696909253)\n",
    "    x = np.linspace(0, 100, 100)\n",
    "    # plt.plot(x, st.gilbrat.pdf(x, params[0], params[1]),'r-', lw=5, alpha=0.6, label='expon pdf')\n",
    "    if len(params)==2:\n",
    "        plt.plot(x, eval(dist).pdf(x, params[0], params[1]),'r-', lw=5, alpha=0.6, label=dist, c=cols[i])\n",
    "    elif len(params)==3:\n",
    "        plt.plot(x, eval(dist).pdf(x, params[0], params[1], params[2]),'r-', lw=5, alpha=0.6, label=dist, c=cols[i])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to iterate through all distributions (did not work for me, copied from stackoverflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import statsmodels as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Create models from data\n",
    "def best_fit_distribution(data, bins=200, ax=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\"\"\"\n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "\n",
    "    # Distributions to check\n",
    "    DISTRIBUTIONS = [        \n",
    "        st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n",
    "        st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n",
    "        st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,\n",
    "        st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,\n",
    "        st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,\n",
    "        st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,\n",
    "        st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,\n",
    "        st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,\n",
    "        st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,\n",
    "        st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy\n",
    "    ]\n",
    "\n",
    "    # Best holders\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for distribution in DISTRIBUTIONS:\n",
    "\n",
    "            # fit dist to data\n",
    "        params = distribution.fit(data)\n",
    "\n",
    "        # Separate parts of parameters\n",
    "        arg = params[:-2]\n",
    "        loc = params[-2]\n",
    "        scale = params[-1]\n",
    "\n",
    "        # Calculate fitted PDF and error with fit in distribution\n",
    "        pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "        sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "        # if axis pass in add to plot\n",
    "        try:\n",
    "            if ax:\n",
    "                pd.Series(pdf, x).plot(ax=ax)\n",
    "            end\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # identify if this distribution is better\n",
    "        if best_sse > sse > 0:\n",
    "            best_distribution = distribution\n",
    "            best_params = params\n",
    "            best_sse = sse\n",
    "            print(best_params)\n",
    "\n",
    "    return (best_distribution.name, best_params)\n",
    "\n",
    "def make_pdf(dist, params, size=10000):\n",
    "    \"\"\"Generate distributions's Probability Distribution Function \"\"\"\n",
    "\n",
    "    # Separate parts of parameters\n",
    "    arg = params[:-2]\n",
    "    loc = params[-2]\n",
    "    scale = params[-1]\n",
    "\n",
    "    # Get sane start and end points of distribution\n",
    "    start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)\n",
    "    end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)\n",
    "\n",
    "    # Build PDF and turn into pandas Series\n",
    "    x = np.linspace(start, end, size)\n",
    "    y = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    pdf = pd.Series(y, x)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "# Load data from statsmodels datasets\n",
    "data = age_vals\n",
    "\n",
    "# Plot for comparison\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.hist(data, bins=50)\n",
    "# Save plot limits\n",
    "\n",
    "# Find best fit distribution\n",
    "best_fit_name, best_fit_params = best_fit_distribution(data, 200, ax)\n",
    "best_dist = getattr(st, best_fit_name)\n",
    "\n",
    "print(best_dist, best_fit_params)\n",
    "# Make PDF with best params \n",
    "pdf = make_pdf(best_dist, best_fit_params)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = pdf.plot(lw=2, label='PDF', legend=True)\n",
    "# data.plot(kind='hist', bins=50, normed=True, alpha=0.5, label='Data', legend=True, ax=ax)\n",
    "plt.hist(data, bins=50)\n",
    "\n",
    "param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']\n",
    "param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, best_fit_params)])\n",
    "dist_str = '{}({})'.format(best_fit_name, param_str)\n",
    "\n",
    "ax.set_title(u'El Niño sea temp. with best fit distribution \\n' + dist_str)\n",
    "ax.set_xlabel(u'Temp. (°C)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
