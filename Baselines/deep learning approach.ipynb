{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from utils import split_inds, mrr, shuffle_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train set: 200\n",
      "Number of users in test set: 30\n",
      "Number of samples including all answer-question pairs: Train: 192162  Test: 26663\n"
     ]
    }
   ],
   "source": [
    "SPLIT_MODE = \"time\"\n",
    "SPLIT = 0.7\n",
    "train_data = \"data\" # \"data_14_1-14_5\" # \"data/\"\n",
    "test_data = \"data_2016\" # \"data_2016_3\" #{ }\"data_15_1-15_5\" # \"data_later\" # \"data_2016\"\n",
    "# data_2016: wrong processig - no user features\n",
    "# data_2016_2: with user features, but some do not have user features because to early\n",
    "# data 2016_3: the ones without the ones with no user features\n",
    "\n",
    "# Load data\n",
    "dataframes = []\n",
    "for file in os.listdir(train_data):\n",
    "    if file[0]==\".\" or \"example\" in file:\n",
    "        continue\n",
    "    df_read = pd.read_csv(os.path.join(train_data,file), index_col=\"id\")\n",
    "    # print(\"Successfully loaded \", file)\n",
    "    dataframes.append(df_read)\n",
    "\n",
    "if SPLIT_MODE==\"user\":\n",
    "    lengths = [len(d) for d in dataframes]\n",
    "    nr_data = sum(lengths)*SPLIT\n",
    "    inds = np.random.permutation(len(lengths))\n",
    "    dataframes_train = []\n",
    "    summed_data = 0\n",
    "    k=0\n",
    "    while summed_data < nr_data:\n",
    "        dataframes_train.append(dataframes[inds[k]])\n",
    "        k+=1\n",
    "        summed_data += lengths[inds[k]]\n",
    "    dataframes_test = [dataframes[j] for j in inds[k:]]\n",
    "    df_train = pd.concat(dataframes_train)\n",
    "    df_test = pd.concat(dataframes_test)\n",
    "    print(\"Number of users in train set:\", len(dataframes_train))\n",
    "    print(\"Number of users in test set:\", len(dataframes_test))\n",
    "    print(\"Number of samples including all answer-question pairs: Train:\", len(df_train), \" Test:\", len(df_test))\n",
    "elif SPLIT_MODE==\"mixed\":\n",
    "    # Take completely random sample (same user might be in test and train set, for different answers)\n",
    "    df = pd.concat(dataframes)\n",
    "    ## Test what values are in question age for ground truth --> mostly 0 or 1 days old, largest 100\n",
    "    # print(len(df))\n",
    "    # gt_df = df.loc[df[\"label\"]==1]\n",
    "    # print(len(gt_df))\n",
    "    # print(np.around(gt_df[\"questionage\"].values, 2))\n",
    "    # Split in train and tests - split by group (one answer-open_questiosn block must be in same part)\n",
    "    df_grouped = df.groupby(\"decision_time\")\n",
    "    # df_train, df_test = split_groups(df_grouped)\n",
    "    nr_groups = len(df_grouped)\n",
    "    train_inds, test_inds = split_inds(nr_groups, split=SPLIT)\n",
    "    df_train = pd.concat([ df_grouped.get_group(group) for i,group in enumerate(df_grouped.groups) if i in train_inds])\n",
    "    df_test = pd.concat([ df_grouped.get_group(group) for i,group in enumerate(df_grouped.groups) if i in test_inds])\n",
    "elif SPLIT_MODE==\"time\":\n",
    "    df_train = pd.concat(dataframes)\n",
    "    dataframes_test = []\n",
    "    for file in os.listdir(test_data):\n",
    "        if file[0]==\".\" or \"example\" in file:\n",
    "            continue\n",
    "        df_read = pd.read_csv(os.path.join(test_data,file), index_col=\"id\")\n",
    "        # print(\"Successfully loaded \", file)\n",
    "        dataframes_test.append(df_read)\n",
    "    df_test = pd.concat(dataframes_test)\n",
    "    print(\"Number of users in train set:\", len(dataframes))\n",
    "    print(\"Number of users in test set:\", len(dataframes_test))\n",
    "    print(\"Number of samples including all answer-question pairs: Train:\", len(df_train), \" Test:\", len(df_test))\n",
    "else:\n",
    "    print(\"ERROR: SPLIT MODE DOES NOT EXIST\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get question body "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "postgres_str = ('postgresql://localhost/crossvalidated')\n",
    "cnx = create_engine(postgres_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_body = pd.read_sql_query('''SELECT Id, body FROM Posts WHERE PostTypeId=1''', cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;How should I elicit prior distributions fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;p&gt;In many different statistical methods there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;What are some valuable Statistical Analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;p&gt;I have two groups of data.  Each with a dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>&lt;p&gt;Last year, I read a blog post from &lt;a href=...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               body\n",
       "0   1  <p>How should I elicit prior distributions fro...\n",
       "1   2  <p>In many different statistical methods there...\n",
       "2   3  <p>What are some valuable Statistical Analysis...\n",
       "3   4  <p>I have two groups of data.  Each with a dif...\n",
       "4   6  <p>Last year, I read a blog post from <a href=..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_body.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26663\n",
      "26663\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.merge(df_train, post_body, how=\"left\", on=\"id\")\n",
    "print(len(df_test))\n",
    "df_test = pd.merge(df_test, post_body, how=\"left\", on=\"id\")\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:  192162  Test set: 26663\n",
      "Class imbalance: 1: 99\n"
     ]
    }
   ],
   "source": [
    "# Prepare training set\n",
    "X_train = df_train.drop(['label', 'decision_time', 'reputation_user', 'reputation_asker', 'body'], axis=1)\n",
    "features = X_train.columns.tolist()\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = np.asarray(X_train)[:, 1:]\n",
    "Y_train = df_train['label'].values\n",
    "G_train = df_train['decision_time'].values\n",
    "B_train = df_train[\"body\"]\n",
    "# print(sorted(np.unique(G_train//100)))\n",
    "\n",
    "# Prepare testing set\n",
    "X_test = df_test.drop(['label', 'decision_time', 'reputation_user', 'reputation_asker', 'body'], axis=1)\n",
    "X_test = np.asarray(X_test)[:, 1:]\n",
    "Y_test = df_test['label'].values\n",
    "G_test = df_test['decision_time'].values\n",
    "B_test = df_test[\"body\"]\n",
    "# print(sorted(np.unique(G_test//100)))\n",
    "assert(len(X_train)==len(Y_train))\n",
    "\n",
    "print(\"Size of training set: \", len(Y_train), \" Test set:\", len(Y_test))\n",
    "class_counts = np.unique(Y_train, return_counts=True)[1]\n",
    "print(\"Class imbalance: 1:\", class_counts[0]//class_counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_4(X,Y,G,B):\n",
    "    assert(len(X)==len(Y))\n",
    "    assert(len(X)==len(G))\n",
    "    assert(len(X)==len(B))\n",
    "    randinds = np.random.permutation(len(Y))\n",
    "    return X[randinds], Y[randinds], G[randinds], B[randinds]\n",
    "\n",
    "X_train, Y_train, G_train, B_train = shuffle_4(X_train, Y_train, G_train, B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(X_train, axis=0)\n",
    "stds = np.std(X_train, axis=0)\n",
    "X_train_norm = (X_train-means)/stds\n",
    "X_test_norm = (X_test-means)/stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning model for text and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, concatenate\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hub layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-es-dim50-with-normalization/1\" # \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding,output_shape=[50], input_shape=[], \n",
    "                           dtype=tf.string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only text: MRR 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 400,373\n",
      "Trainable params: 400,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=opt,\n",
    "              weighted_metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninawiedemann/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 192162 samples, validate on 39243 samples\n",
      "192162/192162 [==============================] - 60s 310us/sample - loss: 0.3395 - accuracy: 0.6639 - val_loss: 0.1309 - val_accuracy: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a1e5841d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(B_train, Y_train, validation_data=(B_test, Y_test), class_weight={0: 1.,1: 50.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(B_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, ranks = mrr(out, G_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058730711053754"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 30\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144354.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = Input(shape=(23))\n",
    "# ip2 = Input(shape=(801,))\n",
    "x = Dense(1024, activation=\"relu\")(ip)\n",
    "# x = Dropout(0.5)(x)\n",
    "x = Dense(256,  activation=\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "out = Dense(1,  activation=\"relu\")(x)\n",
    "\n",
    "model = Model(ip , out)\n",
    "\n",
    "opt = Adam(lr=learning_rate) # , epsilon=None, decay=0.0)\n",
    "# rmsprop = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=opt,\n",
    "              weighted_metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_generator(X_data, Y_data, batch_size):\n",
    "    inds0 = np.where(Y_data==0)[0]\n",
    "    sample0 = np.random.choice(inds0, batch_size//2)\n",
    "    inds1 = np.where(Y_data==1)[0]\n",
    "    sample1 = np.random.choice(inds1, batch_size//2)   \n",
    "    x_batch = np.concatenate((X_data[sample0], X_data[sample1]),axis=0)\n",
    "    y_batch = np.concatenate((Y_data[sample0], Y_data[sample1]),axis=0)\n",
    "    return x_batch, y_batch\n",
    "\n",
    "# x,y = balanced_generator(X_test, Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "          shuffle=True, verbose=1, validation_data=(X_test, Y_test)) # , class_weight={0:1, 1:50})\n",
    "#does not work so far: history = model.fit_generator(balanced_generator(X_train, Y_train,batch_size), epochs=epochs,\n",
    "          # shuffle=True, verbose=1, validation_data=(X_test, Y_test)) # , class_weight={0:1, 1:50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019623742864469614"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.predict(X_test)\n",
    "score, ranks = mrr(out, G_test, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess test: embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192162, 50) (26663, 50)\n"
     ]
    }
   ],
   "source": [
    "B_train_hub = hub_layer(B_train)\n",
    "B_test_hub = hub_layer(B_test)\n",
    "print(B_train_hub.shape, B_test_hub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 23)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 128)          6528        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 128)          3072        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256)          0           dense_25[0][0]                   \n",
      "                                                                 dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1024)         263168      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 256)          262400      dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            257         dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 535,425\n",
      "Trainable params: 535,425\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(B_train_hub.shape[1], ))\n",
    "first_dense = Dense(128, activation='relu')(first_input)\n",
    "\n",
    "second_input = Input(shape=(X_train.shape[1], ))\n",
    "second_dense = Dense(128, activation='relu')(second_input)\n",
    "\n",
    "merge_one = concatenate([first_dense, second_dense])\n",
    "merge_one = Dense(1024, activation='relu')(merge_one)\n",
    "merge_one = Dense(256, activation='relu')(merge_one)\n",
    "out = Dense(1, activation='sigmoid')(merge_one)\n",
    "\n",
    "model = Model(inputs=[first_input, second_input], outputs=out)\n",
    "ada_grad = Adagrad(lr=0.1, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=ada_grad, loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 192162 samples, validate on 39243 samples\n",
      "Epoch 1/2\n",
      "192162/192162 [==============================] - 55s 286us/sample - loss: 0.7549 - accuracy: 0.8172 - val_loss: 1.0628 - val_accuracy: 0.7763\n",
      "Epoch 2/2\n",
      "192162/192162 [==============================] - 54s 279us/sample - loss: 0.7419 - accuracy: 0.8160 - val_loss: 1.1130 - val_accuracy: 0.7906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a21f99f98>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([B_train_hub, X_train_norm], Y_train, validation_data=([B_test_hub, X_test_norm], Y_test), epochs=2, class_weight={0: 1.,1: 100.}) # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24506347603378517"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.predict([B_test_hub, X_test_norm])\n",
    "score, ranks = mrr(out, G_test, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- other embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1 \n",
    "\n",
    "* Train model with 128 / 128 --> concat --> 1024 - 256 - 1 (class weight 1:50) on all features: 0.27\n",
    "* Train model with 128 / 128 --> concat --> 1024 - 256 - 1 (class weight 1:50) on features WITHOUT question age: 0.1 after 10 epochs, 0.11 after 1 epoch\n",
    "\n",
    "Embedding https://tfhub.dev/google/tf2-preview/nnlm-es-dim50-with-normalization/1\n",
    "\n",
    "* Train model with 128 / 128 --> concat --> 1024 - 256 - 1 (class weight 1:50) on features without question age: 0.09 after 1 epoch, 0.11 after 10\n",
    "* Train model with 128 / 128 --> concat --> 1024 - 256 - 1 (CLASS WEIGHT 1:100) on features without question age: 0.12 after 10\n",
    "\n",
    "* Train model with 128 / 128 --> concat --> 1024 - 256 - 1 (CLASS WEIGHT 1:100) on features WITH question age: 0.2755 after 10\n",
    "\n",
    "Use this trained model to compute the score for 2016 data --> 0.245 MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
